{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SP702W4_Katipunan.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"15Texz_iH4MDYdrNsGsYINiKlcMLuPGjQ","authorship_tag":"ABX9TyNHB98sWtGCHsXrA5wWMYaL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4hXwVYJ9FowG"},"source":["# Data Preprocessing Part 2\n","\n","Date created: June 17, 2021\n","\n","Author: A.J. Katipunan\n","\n","Description:\n","\n","This notebook was created as part of the requirements of the Python for Data Engineering Course provided by Coursebank through the Project SPARTA Program.\n","\n","This week's homework is a continuation of the data preprocessing coursework."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDRzDoHT3s1E","executionInfo":{"status":"ok","timestamp":1624370920036,"user_tz":-480,"elapsed":841,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"c10c1289-5817-43a2-e55a-50d367a77cf5"},"source":["import os\n","from google.colab import drive\n","\n","# mount google drive into google collaboratory\n","drive.mount('/content/gdrive')\n","\n","# assign the address of the dataset directory\n","data_dir = '/content/gdrive/MyDrive/GitHub/Python for Data Engineering'\n","\n","# change the working directory\n","os.chdir(data_dir)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OEmiEzjS4ahO","executionInfo":{"status":"ok","timestamp":1624370921290,"user_tz":-480,"elapsed":6,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}}},"source":["username = 'riokatipunan'\n","repository = 'Python_for_data_engineering_SPARTA'\n","git_token = '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'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LBwhYta5_EYN","executionInfo":{"status":"ok","timestamp":1624370984331,"user_tz":-480,"elapsed":1319,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"ac7dcae3-41ea-44ed-ac84-18b24c4d2cb1"},"source":["!git clone https://{git_token}@github.com/{username}/{repository}"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Cloning into 'Python_for_data_engineering_SPARTA'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (3/3), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iaf9kEiH-8Wc","executionInfo":{"status":"ok","timestamp":1624370405301,"user_tz":-480,"elapsed":354,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"18204be1-d49c-41a2-af93-ae5bc9d0a94f"},"source":["%cd {repository}"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/GitHub/Python for Data Engineering/Python_for_data_engineering_SPARTA/Python_for_data_engineering_SPARTA\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"98QstfmV_sE4","executionInfo":{"status":"ok","timestamp":1624370002613,"user_tz":-480,"elapsed":3,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"7c996f9a-6bb1-4e7e-9192-34955ce92f16"},"source":["os.getcwd()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/My Drive/GitHub/Python for Data Engineering'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"bWzH8wNF9OLg","executionInfo":{"status":"ok","timestamp":1624370019267,"user_tz":-480,"elapsed":2263,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}}},"source":["!git config --global user.email \"riokatipunan@gmail.com\"\n","!git config --global user.name \"riokatipunan\""],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gPtZQ-AU4Zo1","executionInfo":{"status":"ok","timestamp":1624370936307,"user_tz":-480,"elapsed":609,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"fe65e2dd-c37d-43d3-be69-96715c67d62a"},"source":["!git status"],"execution_count":4,"outputs":[{"output_type":"stream","text":["fatal: not a git repository (or any parent up to mount point /content)\n","Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FxqhOJMO8olz","executionInfo":{"status":"ok","timestamp":1624370221552,"user_tz":-480,"elapsed":359,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}}},"source":["!git add ."],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBwEIjco84vc","executionInfo":{"status":"ok","timestamp":1624370224437,"user_tz":-480,"elapsed":676,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"e983b2a0-eb41-43c7-f123-c3cf7717c39b"},"source":["!git commit -m \"updated the notebook\""],"execution_count":14,"outputs":[{"output_type":"stream","text":["[main f16a4b4] updated the notebook\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite SP702W4_Katipunan.ipynb (95%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07j_DP4d9tt2","executionInfo":{"status":"ok","timestamp":1624370228590,"user_tz":-480,"elapsed":409,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"310ffade-4831-4b58-df33-e061c058bfcd"},"source":["!git push"],"execution_count":15,"outputs":[{"output_type":"stream","text":["fatal: could not read Password for 'https://l9NuTGIowznqeA4M3SCxd8d8gUiBBHB55JTDlxfodG8@github.com': No such device or address\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BmFgm1-r9fl6"},"source":["--- "]},{"cell_type":"code","metadata":{"id":"pw3l_fjnFXqM"},"source":["# importing necessary modules\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VnE7QHJawvq0"},"source":["## Part 1\n","\n","1. Standardize the price column\n","2. Identify and remove outliers in the dataset"]},{"cell_type":"code","metadata":{"id":"0tMr11tVv7xF"},"source":["# importing the 'sales.csv'\n","sales = pd.read_csv('https://coursebank.ph/assets/courseware/v1/d5196085dd4bb26377be4bd84ccfe39d/asset-v1:DAP+SP702+2020_Q2+type@asset+block/sales.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"ykMBhMepwFKO","executionInfo":{"status":"ok","timestamp":1624355822659,"user_tz":-480,"elapsed":58,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"3dcee57e-bf1a-43e5-dece-f73bbe508e36"},"source":["# describing the sales dataframe\n","sales.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>997.000000</td>\n","      <td>999.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>38.984398</td>\n","      <td>-41.417494</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>19.499041</td>\n","      <td>67.377526</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-41.465000</td>\n","      <td>-159.485280</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>35.782500</td>\n","      <td>-88.207775</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>42.309720</td>\n","      <td>-73.733890</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>51.050000</td>\n","      <td>4.850000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>64.837780</td>\n","      <td>174.766667</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Latitude   Longitude\n","count  997.000000  999.000000\n","mean    38.984398  -41.417494\n","std     19.499041   67.377526\n","min    -41.465000 -159.485280\n","25%     35.782500  -88.207775\n","50%     42.309720  -73.733890\n","75%     51.050000    4.850000\n","max     64.837780  174.766667"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"909bHMzDwJRi","executionInfo":{"status":"ok","timestamp":1624355822661,"user_tz":-480,"elapsed":53,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"a37a5c9f-a885-4f33-f10c-c820684d0a09"},"source":["# print the firts 5 rows of the sales dataframe\n","sales.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Transaction_date</th>\n","      <th>Product</th>\n","      <th>Price</th>\n","      <th>Payment_Type</th>\n","      <th>Name</th>\n","      <th>City</th>\n","      <th>State</th>\n","      <th>Country</th>\n","      <th>Account_Created</th>\n","      <th>Last_Login</th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1/2/2009 6:17</td>\n","      <td>Product1</td>\n","      <td>1200</td>\n","      <td>Mastercard</td>\n","      <td>carolina</td>\n","      <td>Basildon</td>\n","      <td>England</td>\n","      <td>United Kingdom</td>\n","      <td>1/2/2009 6:00</td>\n","      <td>1/2/2009 6:08</td>\n","      <td>51.500000</td>\n","      <td>-1.116667</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1/2/2009 4:53</td>\n","      <td>Product1</td>\n","      <td>1200</td>\n","      <td>Visa</td>\n","      <td>Betina</td>\n","      <td>Parkville</td>\n","      <td>MO</td>\n","      <td>United States</td>\n","      <td>1/2/2009 4:42</td>\n","      <td>1/2/2009 7:49</td>\n","      <td>39.195000</td>\n","      <td>-94.681940</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1/2/2009 13:08</td>\n","      <td>Product1</td>\n","      <td>1200</td>\n","      <td>Mastercard</td>\n","      <td>Federica e Andrea</td>\n","      <td>Astoria</td>\n","      <td>OR</td>\n","      <td>United States</td>\n","      <td>1/1/2009 16:21</td>\n","      <td>1/3/2009 12:32</td>\n","      <td>46.188060</td>\n","      <td>-123.830000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1/3/2009 14:44</td>\n","      <td>Product1</td>\n","      <td>1200</td>\n","      <td>Visa</td>\n","      <td>Gouya</td>\n","      <td>Echuca</td>\n","      <td>Victoria</td>\n","      <td>Australia</td>\n","      <td>9/25/2005 21:13</td>\n","      <td>1/3/2009 14:22</td>\n","      <td>-36.133333</td>\n","      <td>144.750000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1/4/2009 12:56</td>\n","      <td>Product2</td>\n","      <td>3600</td>\n","      <td>Visa</td>\n","      <td>Gerd W</td>\n","      <td>Cahaba Heights</td>\n","      <td>AL</td>\n","      <td>United States</td>\n","      <td>11/15/2008 15:47</td>\n","      <td>1/4/2009 12:45</td>\n","      <td>33.520560</td>\n","      <td>-86.802500</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Transaction_date   Product Price  ...      Last_Login   Latitude   Longitude\n","0    1/2/2009 6:17  Product1  1200  ...   1/2/2009 6:08  51.500000   -1.116667\n","1    1/2/2009 4:53  Product1  1200  ...   1/2/2009 7:49  39.195000  -94.681940\n","2   1/2/2009 13:08  Product1  1200  ...  1/3/2009 12:32  46.188060 -123.830000\n","3   1/3/2009 14:44  Product1  1200  ...  1/3/2009 14:22 -36.133333  144.750000\n","4   1/4/2009 12:56  Product2  3600  ...  1/4/2009 12:45  33.520560  -86.802500\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iWy6FEAXwhAX","executionInfo":{"status":"ok","timestamp":1624355822665,"user_tz":-480,"elapsed":53,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"9201874e-7667-4311-eed4-466e39d16fa9"},"source":["# show the data type of each column in the sales dataframe\n","sales.dtypes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Transaction_date     object\n","Product              object\n","Price                object\n","Payment_Type         object\n","Name                 object\n","City                 object\n","State                object\n","Country              object\n","Account_Created      object\n","Last_Login           object\n","Latitude            float64\n","Longitude           float64\n","dtype: object"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"lVT-3HkPnuuS"},"source":["# drop or remove all duplicates in the sales dataframe\n","sales.drop_duplicates(inplace = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPRKZL-xyh7Q"},"source":["# there are values in the 'Price' column that are not of the type int, but are rather strings\n","# we use this code to remove commas in the 'Price' column and convert the values in this column into float\n","sales['Price'] = sales['Price'].str.replace(',','').astype('float')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gu9LNqgYypyJ"},"source":["# the following block of code removes the outliers in the 'Price' column in the dataframe\n","\n","# the following computes for the mean and stardard deviation\n","mean = np.mean(sales['Price'])\n","std = np.std(sales['Price'])            \n","\n","# then we compute for the lower and upper limits for our dataset\n","lower_limit = max(0, mean - (3*std))\n","upper_limit = mean + (3*std)\n","\n","# then we apply the bounding limits to the sales dataframe\n","# any points outside these bounds will be an outlier\n","sales = sales[(sales['Price'] >= lower_limit)       \n","              &(sales['Price'] <= upper_limit)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"aKRWzGff7wot","executionInfo":{"status":"ok","timestamp":1624355822681,"user_tz":-480,"elapsed":55,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"b0073092-f9f2-450b-8d0b-b35871a3da8d"},"source":["# display the first 5 rows of the cleaned dataframe without any outliersm\n","sales.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Transaction_date</th>\n","      <th>Product</th>\n","      <th>Price</th>\n","      <th>Payment_Type</th>\n","      <th>Name</th>\n","      <th>City</th>\n","      <th>State</th>\n","      <th>Country</th>\n","      <th>Account_Created</th>\n","      <th>Last_Login</th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1/2/2009 6:17</td>\n","      <td>Product1</td>\n","      <td>1200.0</td>\n","      <td>Mastercard</td>\n","      <td>carolina</td>\n","      <td>Basildon</td>\n","      <td>England</td>\n","      <td>United Kingdom</td>\n","      <td>1/2/2009 6:00</td>\n","      <td>1/2/2009 6:08</td>\n","      <td>51.500000</td>\n","      <td>-1.116667</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1/2/2009 4:53</td>\n","      <td>Product1</td>\n","      <td>1200.0</td>\n","      <td>Visa</td>\n","      <td>Betina</td>\n","      <td>Parkville</td>\n","      <td>MO</td>\n","      <td>United States</td>\n","      <td>1/2/2009 4:42</td>\n","      <td>1/2/2009 7:49</td>\n","      <td>39.195000</td>\n","      <td>-94.681940</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1/2/2009 13:08</td>\n","      <td>Product1</td>\n","      <td>1200.0</td>\n","      <td>Mastercard</td>\n","      <td>Federica e Andrea</td>\n","      <td>Astoria</td>\n","      <td>OR</td>\n","      <td>United States</td>\n","      <td>1/1/2009 16:21</td>\n","      <td>1/3/2009 12:32</td>\n","      <td>46.188060</td>\n","      <td>-123.830000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1/3/2009 14:44</td>\n","      <td>Product1</td>\n","      <td>1200.0</td>\n","      <td>Visa</td>\n","      <td>Gouya</td>\n","      <td>Echuca</td>\n","      <td>Victoria</td>\n","      <td>Australia</td>\n","      <td>9/25/2005 21:13</td>\n","      <td>1/3/2009 14:22</td>\n","      <td>-36.133333</td>\n","      <td>144.750000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1/4/2009 12:56</td>\n","      <td>Product2</td>\n","      <td>3600.0</td>\n","      <td>Visa</td>\n","      <td>Gerd W</td>\n","      <td>Cahaba Heights</td>\n","      <td>AL</td>\n","      <td>United States</td>\n","      <td>11/15/2008 15:47</td>\n","      <td>1/4/2009 12:45</td>\n","      <td>33.520560</td>\n","      <td>-86.802500</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Transaction_date   Product   Price  ...      Last_Login   Latitude   Longitude\n","0    1/2/2009 6:17  Product1  1200.0  ...   1/2/2009 6:08  51.500000   -1.116667\n","1    1/2/2009 4:53  Product1  1200.0  ...   1/2/2009 7:49  39.195000  -94.681940\n","2   1/2/2009 13:08  Product1  1200.0  ...  1/3/2009 12:32  46.188060 -123.830000\n","3   1/3/2009 14:44  Product1  1200.0  ...  1/3/2009 14:22 -36.133333  144.750000\n","4   1/4/2009 12:56  Product2  3600.0  ...  1/4/2009 12:45  33.520560  -86.802500\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"bwxpl56R9Glp"},"source":["## Part 2\n","\n","1. Handle all the missing values. Provide an explanation for choosing a specific method(delete, impute, or keep: if impute, why mean, median, or mode?) in handling specific missing values\n","\n","This notebook will explore all methods of handling missing data as discussed in the course (delete, impute, or keep). This for the sake of presenting all possible solutions to the student.\n","\n","Although not required, this notebook furthermore shall also discuss another method of imputation not discussed in the course (i.e. imputation by K-nearest neighbors) that I think will greatly benefit the student who will be able to read this notebook. Further discussions on this are presented below"]},{"cell_type":"code","metadata":{"id":"SV8YVbKP9ZO-"},"source":["# importing 'Handling Missing Values.csv'\n","HMV = pd.read_csv('https://coursebank.ph/assets/courseware/v1/39bd7fa2d5e00e98876d7491cbe61104/asset-v1:DAP+SP702+2020_Q2+type@asset+block/Handling_Missing_Values.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"FZCxV9mx1OIX","executionInfo":{"status":"ok","timestamp":1624355824033,"user_tz":-480,"elapsed":1401,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"1fd8ff6a-290c-4953-e9b6-bccfe9adbdfa"},"source":["# show the first 5 rows of the dataframe\n","HMV.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>Income</th>\n","      <th>Employed</th>\n","      <th>Children</th>\n","      <th>Buy_Car</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Male</td>\n","      <td>25</td>\n","      <td>25146.0</td>\n","      <td>Single</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Male</td>\n","      <td>30</td>\n","      <td>26939.0</td>\n","      <td>Married</td>\n","      <td>2.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Male</td>\n","      <td>27</td>\n","      <td>26693.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Male</td>\n","      <td>28</td>\n","      <td>26666.0</td>\n","      <td>Married</td>\n","      <td>3.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Male</td>\n","      <td>29</td>\n","      <td>25899.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ID   Sex  Age   Income Employed  Children Buy_Car\n","0   1  Male   25  25146.0   Single       0.0      No\n","1   2  Male   30  26939.0  Married       2.0     Yes\n","2   3  Male   27  26693.0  Married       0.0      No\n","3   4  Male   28  26666.0  Married       3.0     Yes\n","4   5  Male   29  25899.0  Married       0.0      No"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGJUJRJ62D6U","executionInfo":{"status":"ok","timestamp":1624355824036,"user_tz":-480,"elapsed":220,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"5369cebc-66e1-46d7-c7cd-485ae0c1087d"},"source":["# show how many records does the dataframe have for each of its columns.\n","# from here we would see that 'Sex', 'Income', and 'Children' have missing values\n","HMV.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ID          20\n","Sex         16\n","Age         20\n","Income      17\n","Employed    20\n","Children    18\n","Buy_Car     20\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"nXmQysglDFZN","executionInfo":{"status":"ok","timestamp":1624355824039,"user_tz":-480,"elapsed":209,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"d5217a71-36d1-4043-b42a-379bcb0f7487"},"source":["# describe the missing values dataframe\n","HMV.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Age</th>\n","      <th>Income</th>\n","      <th>Children</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>20.00000</td>\n","      <td>20.000000</td>\n","      <td>17.000000</td>\n","      <td>18.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>10.50000</td>\n","      <td>28.150000</td>\n","      <td>26245.705882</td>\n","      <td>1.222222</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>5.91608</td>\n","      <td>1.308877</td>\n","      <td>583.052395</td>\n","      <td>1.437136</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.00000</td>\n","      <td>25.000000</td>\n","      <td>25094.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>5.75000</td>\n","      <td>28.000000</td>\n","      <td>26037.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>10.50000</td>\n","      <td>28.000000</td>\n","      <td>26234.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>15.25000</td>\n","      <td>29.000000</td>\n","      <td>26666.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>20.00000</td>\n","      <td>30.000000</td>\n","      <td>26969.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             ID        Age        Income   Children\n","count  20.00000  20.000000     17.000000  18.000000\n","mean   10.50000  28.150000  26245.705882   1.222222\n","std     5.91608   1.308877    583.052395   1.437136\n","min     1.00000  25.000000  25094.000000   0.000000\n","25%     5.75000  28.000000  26037.000000   0.000000\n","50%    10.50000  28.000000  26234.000000   1.000000\n","75%    15.25000  29.000000  26666.000000   2.000000\n","max    20.00000  30.000000  26969.000000   4.000000"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"aGUQKZklkOg1"},"source":["### Part 2.1. Listwise deletion\n","\n","The first method that we will explore would be deleting all rows with incomplete data. This is also called \"listwise deletion\". Performing an analysis with the remaining dataset after listwise deletion is called complete case [1]."]},{"cell_type":"code","metadata":{"id":"yGCV2fPpDMre"},"source":["# we first check if there are columns in our data set that are NaN (not a number), None, or NaT (not a time)\n","nan_cols = [i for i in HMV.columns if HMV[i].isnull().any()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S23mtZnVDh3u","executionInfo":{"status":"ok","timestamp":1624355824045,"user_tz":-480,"elapsed":209,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"ec48c114-d316-4d91-8bfe-02c28ffc2718"},"source":["#  show the list with columns that have rows that might have NaN, None, or Nat\n","nan_cols"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Sex', 'Income', 'Children']"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQYB3Hez0GIx","executionInfo":{"status":"ok","timestamp":1624355824051,"user_tz":-480,"elapsed":207,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"5cf03a3b-c87c-4d35-b6b3-d3e6a9f88030"},"source":["# compute how many rows in the dataframe having missing values\n","HMV.isnull().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ID          0\n","Sex         4\n","Age         0\n","Income      3\n","Employed    0\n","Children    2\n","Buy_Car     0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"id":"jOUoL5INNIHP","executionInfo":{"status":"ok","timestamp":1624355824054,"user_tz":-480,"elapsed":200,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"70bb93ea-ce7e-4189-c7f1-280036723916"},"source":["# identify which rows have NaN as a field value\n","HMV[HMV.isnull().any(axis = 1)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>Income</th>\n","      <th>Employed</th>\n","      <th>Children</th>\n","      <th>Buy_Car</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>Female</td>\n","      <td>28</td>\n","      <td>NaN</td>\n","      <td>Married</td>\n","      <td>3.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>NaN</td>\n","      <td>30</td>\n","      <td>26037.0</td>\n","      <td>Married</td>\n","      <td>2.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>NaN</td>\n","      <td>28</td>\n","      <td>NaN</td>\n","      <td>Single</td>\n","      <td>NaN</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>Male</td>\n","      <td>30</td>\n","      <td>26195.0</td>\n","      <td>Single</td>\n","      <td>NaN</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>NaN</td>\n","      <td>28</td>\n","      <td>26234.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>NaN</td>\n","      <td>28</td>\n","      <td>NaN</td>\n","      <td>Single</td>\n","      <td>0.0</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    ID     Sex  Age   Income Employed  Children Buy_Car\n","6    7  Female   28      NaN  Married       3.0     Yes\n","7    8     NaN   30  26037.0  Married       2.0     Yes\n","9   10     NaN   28      NaN   Single       NaN      No\n","11  12    Male   30  26195.0   Single       NaN     Yes\n","14  15     NaN   28  26234.0  Married       0.0      No\n","17  18     NaN   28      NaN   Single       0.0     Yes"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"TtX5MJOul-Je"},"source":["# drops all rows with missing values (i.e. rows with NaN, None, or NaT ) \n","HMV_to_complete_case = HMV.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"Pt16CX9WoqRj","executionInfo":{"status":"ok","timestamp":1624355824058,"user_tz":-480,"elapsed":199,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"36f5b25b-902e-4b9f-ecb8-63e44afd8617"},"source":["# show the first 5 rows of the complete case of the HMV dataframe \n","HMV_to_complete_case.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>Income</th>\n","      <th>Employed</th>\n","      <th>Children</th>\n","      <th>Buy_Car</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Male</td>\n","      <td>25</td>\n","      <td>25146.0</td>\n","      <td>Single</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Male</td>\n","      <td>30</td>\n","      <td>26939.0</td>\n","      <td>Married</td>\n","      <td>2.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Male</td>\n","      <td>27</td>\n","      <td>26693.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Male</td>\n","      <td>28</td>\n","      <td>26666.0</td>\n","      <td>Married</td>\n","      <td>3.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Male</td>\n","      <td>29</td>\n","      <td>25899.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ID   Sex  Age   Income Employed  Children Buy_Car\n","0   1  Male   25  25146.0   Single       0.0      No\n","1   2  Male   30  26939.0  Married       2.0     Yes\n","2   3  Male   27  26693.0  Married       0.0      No\n","3   4  Male   28  26666.0  Married       3.0     Yes\n","4   5  Male   29  25899.0  Married       0.0      No"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jbou1beG1JeI","executionInfo":{"status":"ok","timestamp":1624355824060,"user_tz":-480,"elapsed":197,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"23370f45-72cc-422e-d0b1-1597bc20a62f"},"source":["# show complete case dataset of the HMV dataframe\n","HMV_to_complete_case.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ID          14\n","Sex         14\n","Age         14\n","Income      14\n","Employed    14\n","Children    14\n","Buy_Car     14\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"3UIu_kpcpkML"},"source":["### Part 2.2. Impuation\n","\n","In statistics, imputation is the process of replacing missing data with substituted values. When substituting for a data point, it is known as \"unit imputation\"; when substituting for a component of a data point, it is known as \"item imputation\". \n","\n","There are three main problems that missing data causes: missing data can introduce a substantial amount of bias, make the handling and analysis of the data more arduous, and create reductions in efficiency.\n","\n","Because missing data can create problems for analyzing data, imputation is seen as a way to avoid pitfalls involved with listwise deletion of cases that have missing values. That is to say, when one or more values are missing for a case, most statistical packages default to discarding any case that has a missing value, which may introduce bias or affect the representativeness of the results. \n","\n","Imputation preserves all cases by replacing missing data with an estimated value based on other available information. Once all missing values have been imputed, the data set can then be analysed using standard techniques for complete data.\n","\n","There have been many theories embraced by scientists to account for missing data but the majority of them introduce bias. A few of the well known attempts to deal with missing data include: hot deck and cold deck imputation; listwise and pairwise deletion; mean imputation; non-negative matrix factorization; regression imputation; last observation carried forward; stochastic imputation; and multiple imputation. The student is advised to review these methods on their own [2].\n","\n","This notebook explores the following imputation methods\n","\n","1. Imputation by mean substitution;\n","2. Imputation by median subsitution;\n","3. Imputation by mode substitution; and\n","4. Imputation using K-nearest neighbors"]},{"cell_type":"markdown","metadata":{"id":"aQgaOMm2rtkv"},"source":["#### Part 2.2.1 Imputation by mean substition\n","\n","This imputation technique involves replacing any missing value with the mean of that variable for all other cases, which has the benefit of not changing the sample mean for that variable. \n","\n","We perform imputation by mean substitution if the variable that we intend to impute with has a normal distribution. \n","\n","However, mean imputation attenuates any correlations involving the variable(s) that are imputed. This is because, in cases with imputation, there is guaranteed to be no relationship between the imputed variable and any other measured variables. \n","\n","Thus, mean imputation has some attractive properties for univariate analysis but becomes problematic for multivariate analysis.\n","\n","Important notes [3]:\n","\n","· If the skewness is between -0.5 is 0.5, the data are fairly symmetrical\n","\n","· If the skewness is between -1 and -is 0.5 or between 0.5 and 1, the data are moderately skewed\n","\n","· If the skewness is less than -1 or greater than 1, the data is highly skewed "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_iFCLKUWFvIn","executionInfo":{"status":"ok","timestamp":1624355824061,"user_tz":-480,"elapsed":189,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"57852b50-da52-4dd8-8d34-a4d5bd822c5b"},"source":["# compute for the skewness of the 'Income' variable\n","# its skewness value would suggest that the 'Income' variable is moderately skewed\n","HMV['Income'].skew()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.7938975768493515"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-r-01ZuiJ0ef","executionInfo":{"status":"ok","timestamp":1624355824065,"user_tz":-480,"elapsed":182,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"d511dbdc-f703-4094-8538-c9d5f669a7ed"},"source":["# compute for the skewness of the 'Children' variable\n","# its skewness value would suggest that 'Children' variable is moderately skewed\n","HMV['Children'].skew()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9006133897460047"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zgc7URHwpbAK","executionInfo":{"status":"ok","timestamp":1624355824068,"user_tz":-480,"elapsed":175,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"87b41278-203c-4868-c130-3489c5f571d8"},"source":["# based on the dicussions in the course, since the data for 'Income' is moderately skewed, \n","# we would usually impute using median substitution, however, for the sake of discussion, let's first impute using mean substitution\n","mean_income = HMV['Income'].mean()\n","HMV['Income'].fillna(mean_income)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0     25146.000000\n","1     26939.000000\n","2     26693.000000\n","3     26666.000000\n","4     25899.000000\n","5     26462.000000\n","6     26245.705882\n","7     26037.000000\n","8     26167.000000\n","9     26245.705882\n","10    26905.000000\n","11    26195.000000\n","12    26514.000000\n","13    26969.000000\n","14    26234.000000\n","15    26229.000000\n","16    25094.000000\n","17    26245.705882\n","18    26601.000000\n","19    25427.000000\n","Name: Income, dtype: float64"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n3ejthAfKQwj","executionInfo":{"status":"ok","timestamp":1624355824070,"user_tz":-480,"elapsed":169,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"a5168ff2-5ffc-4524-f40e-c48e12e5df76"},"source":["# based on the dicussions in the course, since the data for 'Children' is moderately skewed, \n","# we would usually impute using median substitution, however, for the sake of discussion, let's first impute using mean substitution\n","mean_children = HMV['Children'].mean()\n","HMV_mean_children = HMV['Children'].fillna(mean_children)\n","HMV_mean_children.apply(np.ceil)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0     0.0\n","1     2.0\n","2     0.0\n","3     3.0\n","4     0.0\n","5     1.0\n","6     3.0\n","7     2.0\n","8     1.0\n","9     2.0\n","10    0.0\n","11    2.0\n","12    4.0\n","13    4.0\n","14    0.0\n","15    0.0\n","16    0.0\n","17    0.0\n","18    1.0\n","19    1.0\n","Name: Children, dtype: float64"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"rs6iAJ75_djJ"},"source":["#### Part 2.2.2 Imputation by median substition\n","\n","This imputation technique involves replacing any missing value with the median of that variable for all other cases. \n","\n","We perform imputation by median substitution if the variable that we intend to impute with does not have a normal distribution. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXZXm6Vq_Z5v","executionInfo":{"status":"ok","timestamp":1624355824072,"user_tz":-480,"elapsed":164,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"5a0b8d31-8b8e-4d8c-dda8-04974797c30b"},"source":["# as shown in the previous cells, the data for 'Income' is moderately skewed, \n","# therefore imputation by median substitution would be the better method of imputation.\n","median_income = HMV['Income'].median()\n","HMV['Income'].fillna(median_income)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0     25146.0\n","1     26939.0\n","2     26693.0\n","3     26666.0\n","4     25899.0\n","5     26462.0\n","6     26234.0\n","7     26037.0\n","8     26167.0\n","9     26234.0\n","10    26905.0\n","11    26195.0\n","12    26514.0\n","13    26969.0\n","14    26234.0\n","15    26229.0\n","16    25094.0\n","17    26234.0\n","18    26601.0\n","19    25427.0\n","Name: Income, dtype: float64"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slHCnQWkJ6Fr","executionInfo":{"status":"ok","timestamp":1624355824074,"user_tz":-480,"elapsed":158,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"30545b63-183d-458f-df55-2662a203f2ab"},"source":["# as shown in the previous cells, the data for 'Children' is moderately skewed, \n","# therefore imputation by median substitution would be the better method of imputation.\n","children_income = HMV['Children'].median()\n","HMV['Children'].fillna(children_income)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0     0.0\n","1     2.0\n","2     0.0\n","3     3.0\n","4     0.0\n","5     1.0\n","6     3.0\n","7     2.0\n","8     1.0\n","9     1.0\n","10    0.0\n","11    1.0\n","12    4.0\n","13    4.0\n","14    0.0\n","15    0.0\n","16    0.0\n","17    0.0\n","18    1.0\n","19    1.0\n","Name: Children, dtype: float64"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"g4pqAK07AlXn"},"source":["#### Part 2.2.3 Imputation by mode substition\n","\n","This imputation technique involves replacing any missing value with the mode of that variable for all other cases.\n","\n","We perform imputation by mode substitution if the variable that we intend to impute is composed of categorical data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n_dI2gwjExOH","executionInfo":{"status":"ok","timestamp":1624355824076,"user_tz":-480,"elapsed":148,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"7e4364f9-fd9a-446b-ff11-f5f8f9ff0f34"},"source":["# imputing the 'Sex' variable using mode substitution\n","mode_sex = HMV['Sex'].mode()\n","HMV['Sex'].fillna(mode_sex[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       Male\n","1       Male\n","2       Male\n","3       Male\n","4       Male\n","5       Male\n","6     Female\n","7       Male\n","8     Female\n","9       Male\n","10    Female\n","11      Male\n","12    Female\n","13    Female\n","14      Male\n","15      Male\n","16    Female\n","17      Male\n","18      Male\n","19      Male\n","Name: Sex, dtype: object"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FOkP1gsELotV","executionInfo":{"status":"ok","timestamp":1624355824079,"user_tz":-480,"elapsed":140,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"4141d1b5-15cd-41b3-f090-a7179329bfda"},"source":["# imputing the 'Sex' variable using mode substitution\n","mode_children = HMV['Children'].mode()\n","HMV['Children'].fillna(mode_children[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0     0.0\n","1     2.0\n","2     0.0\n","3     3.0\n","4     0.0\n","5     1.0\n","6     3.0\n","7     2.0\n","8     1.0\n","9     0.0\n","10    0.0\n","11    0.0\n","12    4.0\n","13    4.0\n","14    0.0\n","15    0.0\n","16    0.0\n","17    0.0\n","18    1.0\n","19    1.0\n","Name: Children, dtype: float64"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"9LRdtyWayO0I"},"source":["#### Part 2.2.3 K-nearest neighbors imputation\n","\n","The k nearest neighbors algorithm can be used for imputing missing data by finding the k closest neighbors to the observation with missing data and then imputing them based on the the non-missing values in the neighbors. There are several possible approaches to this. You can use 1NN schema, where you find the most similar neighbor and then use its value as a missing data replacement. Alternatively you can use kNN, with k neighbors and take mean of the neighbors, or weighted mean, where the distances to neighbors are used as weights, so the closer neighbor is, the more weight it has when taking the mean. Using weighted mean seems to be used most commonly.\n","\n","The KNNImputer class of scikit-learn package provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed. When the number of available neighbors is less than n_neighbors and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during transform.\n","\n","You can refer to this [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959387/) and [article](https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637) in Medium for more information"]},{"cell_type":"code","metadata":{"id":"TGioFvtszcIE"},"source":["# before we can perform imputation by k-nearest neighbors, we first need to map all categorical variables into a corresponding numerical value\n","# so that our machine could perform matrix operations to the dataframe\n","\n","# This code maps the 'Sex' variable; 'Male' would be 1, 'Female' would be 2\n","HMV['Sex'] = HMV['Sex'].map({'Male':1, \n","                             'Female':2})\n","\n","# This code maps the 'Employed' variable; 'Single' would be 1, 'Married' would be 2\n","HMV['Employed'] = HMV['Employed'].map({'Single':1,\n","                                       'Married': 2})\n","\n","# This code maps the 'Buy_Car' variable; 'Yes' would be 1, 'No' would be 2\n","HMV['Buy_Car'] = HMV['Buy_Car'].map({'Yes':1,\n","                                     'No': 2})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":665},"id":"FvpOn-ob6m-k","executionInfo":{"status":"ok","timestamp":1624355824084,"user_tz":-480,"elapsed":135,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"029cd61b-fa9f-4d20-a288-f98fc0fafaf5"},"source":["# show the mapped dataframe\n","HMV"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>Income</th>\n","      <th>Employed</th>\n","      <th>Children</th>\n","      <th>Buy_Car</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>25</td>\n","      <td>25146.0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>30</td>\n","      <td>26939.0</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1.0</td>\n","      <td>27</td>\n","      <td>26693.0</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1.0</td>\n","      <td>28</td>\n","      <td>26666.0</td>\n","      <td>2</td>\n","      <td>3.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1.0</td>\n","      <td>29</td>\n","      <td>25899.0</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>1.0</td>\n","      <td>28</td>\n","      <td>26462.0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>2.0</td>\n","      <td>28</td>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>3.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>NaN</td>\n","      <td>30</td>\n","      <td>26037.0</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>2.0</td>\n","      <td>28</td>\n","      <td>26167.0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>NaN</td>\n","      <td>28</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>2.0</td>\n","      <td>28</td>\n","      <td>26905.0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>1.0</td>\n","      <td>30</td>\n","      <td>26195.0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>2.0</td>\n","      <td>30</td>\n","      <td>26514.0</td>\n","      <td>2</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>2.0</td>\n","      <td>28</td>\n","      <td>26969.0</td>\n","      <td>2</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>NaN</td>\n","      <td>28</td>\n","      <td>26234.0</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>1.0</td>\n","      <td>27</td>\n","      <td>26229.0</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>2.0</td>\n","      <td>26</td>\n","      <td>25094.0</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>NaN</td>\n","      <td>28</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>1.0</td>\n","      <td>28</td>\n","      <td>26601.0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>1.0</td>\n","      <td>29</td>\n","      <td>25427.0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    ID  Sex  Age   Income  Employed  Children  Buy_Car\n","0    1  1.0   25  25146.0         1       0.0        2\n","1    2  1.0   30  26939.0         2       2.0        1\n","2    3  1.0   27  26693.0         2       0.0        2\n","3    4  1.0   28  26666.0         2       3.0        1\n","4    5  1.0   29  25899.0         2       0.0        2\n","5    6  1.0   28  26462.0         2       1.0        2\n","6    7  2.0   28      NaN         2       3.0        1\n","7    8  NaN   30  26037.0         2       2.0        1\n","8    9  2.0   28  26167.0         2       1.0        1\n","9   10  NaN   28      NaN         1       NaN        2\n","10  11  2.0   28  26905.0         1       0.0        2\n","11  12  1.0   30  26195.0         1       NaN        1\n","12  13  2.0   30  26514.0         2       4.0        1\n","13  14  2.0   28  26969.0         2       4.0        1\n","14  15  NaN   28  26234.0         2       0.0        2\n","15  16  1.0   27  26229.0         2       0.0        2\n","16  17  2.0   26  25094.0         2       0.0        2\n","17  18  NaN   28      NaN         1       0.0        1\n","18  19  1.0   28  26601.0         2       1.0        2\n","19  20  1.0   29  25427.0         2       1.0        2"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"r5X4SEk1yVpu"},"source":["# with the HMV dataframe now composed of numerical values, we can now peform imputation by k-nearest neighbors\n","\n","# we import the KNNImputer class from sklearn\n","from sklearn.impute import KNNImputer\n","\n","# we initialize the KNNImputer class; we use the 5 nearest neighbors for the imputation with equal weights\n","imputer = KNNImputer(n_neighbors = 5, weights = 'uniform')\n","\n","# we perform the KNN imputation \n","HMV_KNNImputation = imputer.fit_transform(HMV, y = None)\n","\n","# HMV_KNNImputation is actually a numpy array; we therefore need to convert it into a pandas dataframe and apply the corresponding column names\n","HMV_KNNImputation = pd.DataFrame(HMV_KNNImputation, columns = ['ID', 'Sex', 'Age', 'Income', 'Employed', 'Children', 'Buy_Car'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":665},"id":"gInMPMpC6Kf7","executionInfo":{"status":"ok","timestamp":1624355824087,"user_tz":-480,"elapsed":132,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"8f9b3a7c-9442-4e0b-d086-42fff8d09fda"},"source":["# this cell shows the impuated dataframe; As you would see the field values of some of the variables are still in their mapped numerical values; \n","# we need to revert them back to their original assignments\n","HMV_KNNImputation"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>Income</th>\n","      <th>Employed</th>\n","      <th>Children</th>\n","      <th>Buy_Car</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>25.0</td>\n","      <td>25146.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>30.0</td>\n","      <td>26939.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>27.0</td>\n","      <td>26693.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>28.0</td>\n","      <td>26666.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>29.0</td>\n","      <td>25899.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","      <td>28.0</td>\n","      <td>26462.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>28.0</td>\n","      <td>26246.2</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8.0</td>\n","      <td>1.4</td>\n","      <td>30.0</td>\n","      <td>26037.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>28.0</td>\n","      <td>26167.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10.0</td>\n","      <td>1.8</td>\n","      <td>28.0</td>\n","      <td>26363.6</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11.0</td>\n","      <td>2.0</td>\n","      <td>28.0</td>\n","      <td>26905.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12.0</td>\n","      <td>1.0</td>\n","      <td>30.0</td>\n","      <td>26195.0</td>\n","      <td>1.0</td>\n","      <td>0.8</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13.0</td>\n","      <td>2.0</td>\n","      <td>30.0</td>\n","      <td>26514.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14.0</td>\n","      <td>2.0</td>\n","      <td>28.0</td>\n","      <td>26969.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15.0</td>\n","      <td>1.4</td>\n","      <td>28.0</td>\n","      <td>26234.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16.0</td>\n","      <td>1.0</td>\n","      <td>27.0</td>\n","      <td>26229.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17.0</td>\n","      <td>2.0</td>\n","      <td>26.0</td>\n","      <td>25094.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18.0</td>\n","      <td>1.4</td>\n","      <td>28.0</td>\n","      <td>25917.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19.0</td>\n","      <td>1.0</td>\n","      <td>28.0</td>\n","      <td>26601.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20.0</td>\n","      <td>1.0</td>\n","      <td>29.0</td>\n","      <td>25427.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      ID  Sex   Age   Income  Employed  Children  Buy_Car\n","0    1.0  1.0  25.0  25146.0       1.0       0.0      2.0\n","1    2.0  1.0  30.0  26939.0       2.0       2.0      1.0\n","2    3.0  1.0  27.0  26693.0       2.0       0.0      2.0\n","3    4.0  1.0  28.0  26666.0       2.0       3.0      1.0\n","4    5.0  1.0  29.0  25899.0       2.0       0.0      2.0\n","5    6.0  1.0  28.0  26462.0       2.0       1.0      2.0\n","6    7.0  2.0  28.0  26246.2       2.0       3.0      1.0\n","7    8.0  1.4  30.0  26037.0       2.0       2.0      1.0\n","8    9.0  2.0  28.0  26167.0       2.0       1.0      1.0\n","9   10.0  1.8  28.0  26363.6       1.0       2.0      2.0\n","10  11.0  2.0  28.0  26905.0       1.0       0.0      2.0\n","11  12.0  1.0  30.0  26195.0       1.0       0.8      1.0\n","12  13.0  2.0  30.0  26514.0       2.0       4.0      1.0\n","13  14.0  2.0  28.0  26969.0       2.0       4.0      1.0\n","14  15.0  1.4  28.0  26234.0       2.0       0.0      2.0\n","15  16.0  1.0  27.0  26229.0       2.0       0.0      2.0\n","16  17.0  2.0  26.0  25094.0       2.0       0.0      2.0\n","17  18.0  1.4  28.0  25917.0       1.0       0.0      1.0\n","18  19.0  1.0  28.0  26601.0       2.0       1.0      2.0\n","19  20.0  1.0  29.0  25427.0       2.0       1.0      2.0"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"2ySlrt3u7ZHZ"},"source":["# reviewing the imputed dataframe, it would seem that some imputed values in the 'Sex' variable has values between 1 and 2 \n","# (e.g. check row with ID no. 8, 10, and 18); We can correct this by setting an conditional that if the variable is less than or equal to 1.5, \n","# then the value would be reassigned to 1; else, we assign it with a value of 2 (ideally, if the value is equal to 1.5, \n","# we should randomly select among 1 and 2 to be the value for the 'Sex' variable)\n","HMV_KNNImputation['Sex'] = HMV_KNNImputation['Sex'].apply(lambda x: 1 if x >=1.5 else 2)\n","\n","# This code maps the 'Sex' variable; 1 would be 'Male', 2 would be 'Female'\n","HMV_KNNImputation['Sex'] = HMV_KNNImputation['Sex'].map({ 1 : 'Male', \n","                                                          2 : 'Female'})\n","\n","# This code maps the 'Employed' variable; 1 would be 'Single', 2 would be 'Married'\n","HMV_KNNImputation['Employed'] = HMV_KNNImputation['Employed'].map({ 1 : 'Single',\n","                                                                    2 : 'Married'})\n","\n","# reviewing the imputed dataframe, it would also seem that some imputed values in the 'Children' variable does not have discret or whole number values \n","# (e.g. check row with ID no. 12); \n","# We can correct this by setting getting it's ceiling value, i.e. we round up. (e.g. if the value is 0.8, we round up to get 1)\n","HMV_KNNImputation['Children'] = HMV_KNNImputation['Children'].apply(lambda x: np.ceil(x))\n","\n","# This code maps the 'Buy_Car' variable; 1 would be 'Yes', 2 would be 'No'\n","HMV_KNNImputation['Buy_Car'] = HMV_KNNImputation['Buy_Car'].map({ 1 : 'Yes',\n","                                                                  2 : 'No'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":665},"id":"RyhCAkLA8mMv","executionInfo":{"status":"ok","timestamp":1624355824091,"user_tz":-480,"elapsed":130,"user":{"displayName":"Rio Katipunan","photoUrl":"","userId":"05212169657949591853"}},"outputId":"9fdd51ab-025d-4df4-eae2-c6ca121b610b"},"source":["# show the imputed and re-mapped dataframe\n","HMV_KNNImputation"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>Income</th>\n","      <th>Employed</th>\n","      <th>Children</th>\n","      <th>Buy_Car</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>Female</td>\n","      <td>25.0</td>\n","      <td>25146.0</td>\n","      <td>Single</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.0</td>\n","      <td>Female</td>\n","      <td>30.0</td>\n","      <td>26939.0</td>\n","      <td>Married</td>\n","      <td>2.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3.0</td>\n","      <td>Female</td>\n","      <td>27.0</td>\n","      <td>26693.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.0</td>\n","      <td>Female</td>\n","      <td>28.0</td>\n","      <td>26666.0</td>\n","      <td>Married</td>\n","      <td>3.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>Female</td>\n","      <td>29.0</td>\n","      <td>25899.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6.0</td>\n","      <td>Female</td>\n","      <td>28.0</td>\n","      <td>26462.0</td>\n","      <td>Married</td>\n","      <td>1.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7.0</td>\n","      <td>Male</td>\n","      <td>28.0</td>\n","      <td>26246.2</td>\n","      <td>Married</td>\n","      <td>3.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8.0</td>\n","      <td>Female</td>\n","      <td>30.0</td>\n","      <td>26037.0</td>\n","      <td>Married</td>\n","      <td>2.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9.0</td>\n","      <td>Male</td>\n","      <td>28.0</td>\n","      <td>26167.0</td>\n","      <td>Married</td>\n","      <td>1.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10.0</td>\n","      <td>Male</td>\n","      <td>28.0</td>\n","      <td>26363.6</td>\n","      <td>Single</td>\n","      <td>2.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11.0</td>\n","      <td>Male</td>\n","      <td>28.0</td>\n","      <td>26905.0</td>\n","      <td>Single</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12.0</td>\n","      <td>Female</td>\n","      <td>30.0</td>\n","      <td>26195.0</td>\n","      <td>Single</td>\n","      <td>1.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13.0</td>\n","      <td>Male</td>\n","      <td>30.0</td>\n","      <td>26514.0</td>\n","      <td>Married</td>\n","      <td>4.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14.0</td>\n","      <td>Male</td>\n","      <td>28.0</td>\n","      <td>26969.0</td>\n","      <td>Married</td>\n","      <td>4.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15.0</td>\n","      <td>Female</td>\n","      <td>28.0</td>\n","      <td>26234.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16.0</td>\n","      <td>Female</td>\n","      <td>27.0</td>\n","      <td>26229.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17.0</td>\n","      <td>Male</td>\n","      <td>26.0</td>\n","      <td>25094.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18.0</td>\n","      <td>Female</td>\n","      <td>28.0</td>\n","      <td>25917.0</td>\n","      <td>Single</td>\n","      <td>0.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19.0</td>\n","      <td>Female</td>\n","      <td>28.0</td>\n","      <td>26601.0</td>\n","      <td>Married</td>\n","      <td>1.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20.0</td>\n","      <td>Female</td>\n","      <td>29.0</td>\n","      <td>25427.0</td>\n","      <td>Married</td>\n","      <td>1.0</td>\n","      <td>No</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      ID     Sex   Age   Income Employed  Children Buy_Car\n","0    1.0  Female  25.0  25146.0   Single       0.0      No\n","1    2.0  Female  30.0  26939.0  Married       2.0     Yes\n","2    3.0  Female  27.0  26693.0  Married       0.0      No\n","3    4.0  Female  28.0  26666.0  Married       3.0     Yes\n","4    5.0  Female  29.0  25899.0  Married       0.0      No\n","5    6.0  Female  28.0  26462.0  Married       1.0      No\n","6    7.0    Male  28.0  26246.2  Married       3.0     Yes\n","7    8.0  Female  30.0  26037.0  Married       2.0     Yes\n","8    9.0    Male  28.0  26167.0  Married       1.0     Yes\n","9   10.0    Male  28.0  26363.6   Single       2.0      No\n","10  11.0    Male  28.0  26905.0   Single       0.0      No\n","11  12.0  Female  30.0  26195.0   Single       1.0     Yes\n","12  13.0    Male  30.0  26514.0  Married       4.0     Yes\n","13  14.0    Male  28.0  26969.0  Married       4.0     Yes\n","14  15.0  Female  28.0  26234.0  Married       0.0      No\n","15  16.0  Female  27.0  26229.0  Married       0.0      No\n","16  17.0    Male  26.0  25094.0  Married       0.0      No\n","17  18.0  Female  28.0  25917.0   Single       0.0     Yes\n","18  19.0  Female  28.0  26601.0  Married       1.0      No\n","19  20.0  Female  29.0  25427.0  Married       1.0      No"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"BMd_EHR6BAno"},"source":["## Part 3\n","\n","1. Gather data from any source\n","2. Preprocess the data the gathered data"]},{"cell_type":"code","metadata":{"id":"7SrL0fOuBL2v"},"source":["# import ssl\n","# ssl._create_default_https_context = ssl._create_unverified_context\n","# earthquake_data = pd.read_html('https://www.phivolcs.dost.gov.ph/index.php/earthquake/earthquake-information3')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"agShgURDBRJ0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJx-SD2ik0Hk"},"source":["## References \n","\n","\n","[1] https://en.wikipedia.org/wiki/Listwise_deletion\n","\n","[2] https://en.wikipedia.org/wiki/Imputation_(statistics)\n","\n","[3] https://medium.com/@atanudan/kurtosis-skew-function-in-pandas-aa63d72e20de"]},{"cell_type":"code","metadata":{"id":"OIv5zKm6k2_2"},"source":[""],"execution_count":null,"outputs":[]}]}